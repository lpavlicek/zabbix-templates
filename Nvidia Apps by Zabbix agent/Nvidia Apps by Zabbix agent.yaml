zabbix_export:
  version: '7.4'
  template_groups:
    - uuid: a1c09fe31fae4b56b41195ab9b4910ff
      name: 'lpavlicek templates'
  templates:
    - uuid: 7aad7adac9f344f393050beecfb59b02
      template: 'Nvidia Apps by Zabbix agent'
      name: 'Nvidia Apps by Zabbix agent'
      description: |
        Monitors NVIDIA GPU compute applications (processes) via nvidia-smi.
        
        Requires a Zabbix agent UserParameter on the monitored host:
        
          UserParameter=nvidia.apps.json,/etc/zabbix/scripts/nvidia/nvidia_apps_json.py
        
        The script returns a JSON object with error status and per-GPU process count and oldest process age (in seconds).
        
        Expected JSON format:
            {
               "error": null,
               "gpus": {
                      "GPU-xxxx": {"proc_count": 2, "max_age": 19189},
                      ...
               }
            }
      vendor:
        name: lpavlicek
        version: 7.4-1
      groups:
        - name: 'lpavlicek templates'
      items:
        - uuid: 972b9b78ad6e4eeabdb8544395a1c0e5
          name: 'Nvidia: get apps error status'
          type: DEPENDENT
          key: nvidia.apps.error_status
          value_type: TEXT
          description: |
            Reflects the availability of the nvidia-smi command on the host.
            
            Value is "null" when nvidia-smi runs successfully. Any other value contains the error message returned by the script (e.g. "NVML Driver/library version mismatch").
          preprocessing:
            - type: JSONPATH
              parameters:
                - $.error
            - type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 4h
          master_item:
            key: nvidia.apps.json
          tags:
            - tag: component
              value: nvidia
          triggers:
            - uuid: 8cda46d2dffa4a22bd9c9a9c62882724
              expression: 'last(/Nvidia Apps by Zabbix agent/nvidia.apps.error_status)<> "null"'
              name: 'Nvidia get apps (nvidia-smi) error status'
              opdata: '{ITEM.VALUE}'
              priority: HIGH
              description: |
                nvidia-smi returned an error or could not be executed.
                
                Common cause: NVIDIA NVML Driver/library version mismatch after a kernel or driver update.
                
                === TROUBLESHOOTING ===
                
                1) Check the kernel driver version:
                       cat /proc/driver/nvidia/version
                
                2) Update initramfs for the current kernel:
                        update-initramfs -u
                
                3) Try reloading NVIDIA kernel modules (only when GPU is idle):
                         lsmod | grep nvidia
                         sudo rmmod nvidia_drm nvidia_modeset nvidia_uvm nvidia
                         nvidia-smi
                
                4) If the GPU is in use, a reboot after step 2 is required.
              tags:
                - tag: component
                  value: nvidia
        - uuid: a6a90e908bf649fe8eb824adc349e3d0
          name: 'Nvidia: get apps raw data'
          key: nvidia.apps.json
          delay: 5m
          history: 1h
          value_type: TEXT
          description: 'Master item. Executes the nvidia_apps_json.py script on the monitored host via Zabbix agent UserParameter and stores the raw JSON output. All other items and LLD rules depend on this item.'
          tags:
            - tag: component
              value: nvidia
            - tag: stage
              value: raw
      discovery_rules:
        - uuid: 36fd9f77d6ae441fb1a4fa02d5649bd2
          name: 'Nvidia: GPU discovery'
          type: DEPENDENT
          key: nvidia.apps.gpu.discovery
          description: |
            Discovers NVIDIA GPUs that currently have at least one compute process attached. One set of item prototypes is created per GPU.
            
            Note: a GPU disappears from discovery when all its processes end. Existing items are kept according to the "Keep lost resources" setting.
          item_prototypes:
            - uuid: 14844a492fae4856902307340f3fe84f
              name: '[{#GPU_UUID}]: Compute apps status'
              type: DEPENDENT
              key: 'nvidia.apps.gpu_status[{#GPU_UUID}]'
              description: |
                Aggregated status of the GPU compute workload.
                
                0 – Idle: no compute processes attached
                2 – Warning (info): oldest process exceeds {$GPU_PROC_MAX_AGE_LIMIT_INFO}
                3 – Warning: oldest process exceeds {$GPU_PROC_MAX_AGE_LIMIT_WARN}
                4 – Error: nvidia-smi failed or returned invalid data
              valuemap:
                name: 'Nvidia GPU compute status'
              preprocessing:
                - type: JAVASCRIPT
                  parameters:
                    - |
                      var gpu_uuid   = "{#GPU_UUID}";
                      var limit_info = "{$GPU_PROC_MAX_AGE_LIMIT_INFO}";
                      var limit_warn = "{$GPU_PROC_MAX_AGE_LIMIT_WARN}";
                      
                      // --- Pomocná funkce: převod časového řetězce na sekundy ---
                      function parseTimeToSeconds(t) {
                          t = String(t).trim();
                          var match = t.match(/^(\d+(?:\.\d+)?)\s*([smhd]?)$/i);
                          if (!match) return NaN;
                          var val  = parseFloat(match[1]);
                          var unit = match[2].toLowerCase();
                          switch (unit) {
                              case 's': case '': return val;
                              case 'm': return val * 60;
                              case 'h': return val * 3600;
                              case 'd': return val * 86400;
                              default:  return NaN;
                          }
                      }
                      
                      // --- Parse JSON ---
                      var data;
                      try {
                          data = JSON.parse(value);
                      } catch (e) {
                          return 4; // Chyba parsování = stav 4
                      }
                      
                      // --- Stav 4: chyba nahlášená skriptem ---
                      if (data.error !== null && data.error !== undefined) {
                          return 4;
                      }
                      
                      // --- Najdi GPU ---
                      var gpu = data.gpus ? data.gpus[gpu_uuid] : undefined;
                      if (gpu === undefined) {
                          // GPU není v JSONu – buď žádné procesy, nebo UUID nesedí
                          // Vracíme 0 (žádná aplikace), ne chybu
                          return 0;
                      }
                      
                      // --- Stav 0: žádné procesy ---
                      if (!gpu.proc_count || gpu.proc_count === 0) {
                          return 0;
                      }
                      
                      // --- Stav 1–3 podle stáří nejstaršího procesu ---
                      var max_age   = gpu.max_age;                  // sekundy
                      var sec_warn  = parseTimeToSeconds(limit_warn);
                      var sec_info  = parseTimeToSeconds(limit_info);
                      
                      if (!isNaN(sec_warn) && max_age > sec_warn) {
                          return 3;
                      }
                      if (!isNaN(sec_info) && max_age > sec_info) {
                          return 2;
                      }
                      return 1;
                - type: DISCARD_UNCHANGED_HEARTBEAT
                  parameters:
                    - 1h
              master_item:
                key: nvidia.apps.json
              tags:
                - tag: component
                  value: nvidia
                - tag: device
                  value: '{#GPU_UUID}'
            - uuid: 64f22f8146c94430aed005481a2d78d4
              name: '[{#GPU_UUID}]: Oldest compute app age'
              type: DEPENDENT
              key: 'nvidia.apps.max_age[{#GPU_UUID}]'
              units: s
              description: |
                Age (in seconds) of the longest-running compute process currently using this GPU.
                
                The script determines process age from /proc/<pid>/stat (process start time relative to system boot).
                
                Note: the age resets to zero when all processes detach and a new process attaches. It measures continuous GPU occupancy per process, not per GPU session.
              preprocessing:
                - type: JSONPATH
                  parameters:
                    - '$.gpus["{#GPU_UUID}"].max_age'
              master_item:
                key: nvidia.apps.json
              tags:
                - tag: component
                  value: nvidia
                - tag: device
                  value: '{#GPU_UUID}'
              trigger_prototypes:
                - uuid: 843354ce2560431ea213c0ce5dcc0ee3
                  expression: 'last(/Nvidia Apps by Zabbix agent/nvidia.apps.max_age[{#GPU_UUID}]) > {$GPU_PROC_MAX_AGE_LIMIT_INFO}'
                  name: '[{#GPU_UUID}]: Max process age > {$GPU_PROC_MAX_AGE_LIMIT_INFO}'
                  opdata: '{ITEM.VALUE}'
                  priority: INFO
                  description: |
                    A compute process has been using this GPU continuously for longer than {$GPU_PROC_MAX_AGE_LIMIT_INFO}.
                    
                    This may indicate a forgotten or stuck job. Review running.
                  manual_close: 'YES'
                  dependencies:
                    - name: 'Nvidia get apps (nvidia-smi) error status'
                      expression: 'last(/Nvidia Apps by Zabbix agent/nvidia.apps.error_status)<> "null"'
                    - name: '[{#GPU_UUID}]: Max process age > {$GPU_PROC_MAX_AGE_LIMIT_WARN}'
                      expression: 'last(/Nvidia Apps by Zabbix agent/nvidia.apps.max_age[{#GPU_UUID}]) > {$GPU_PROC_MAX_AGE_LIMIT_WARN}'
                  tags:
                    - tag: component
                      value: nvidia
                - uuid: d2fcaf691d3b4ada90893e3e8349c472
                  expression: 'last(/Nvidia Apps by Zabbix agent/nvidia.apps.max_age[{#GPU_UUID}]) > {$GPU_PROC_MAX_AGE_LIMIT_WARN}'
                  name: '[{#GPU_UUID}]: Max process age > {$GPU_PROC_MAX_AGE_LIMIT_WARN}'
                  opdata: '{ITEM.VALUE}'
                  priority: WARNING
                  description: |
                    A compute process has been using this GPU continuously for longer than {$GPU_PROC_MAX_AGE_LIMIT_WARN}.
                    
                    This likely indicates a long-running or stuck job.
                  manual_close: 'YES'
                  dependencies:
                    - name: 'Nvidia get apps (nvidia-smi) error status'
                      expression: 'last(/Nvidia Apps by Zabbix agent/nvidia.apps.error_status)<> "null"'
                  tags:
                    - tag: component
                      value: nvidia
            - uuid: 09bcb17b8b0b486387f6d3019ecff33f
              name: '[{#GPU_UUID}]: Number of compute apps'
              type: DEPENDENT
              key: 'nvidia.apps.proc_count[{#GPU_UUID}]'
              description: 'Number of compute processes (applications) currently using this GPU. Sourced from "nvidia-smi --query-compute-apps".'
              preprocessing:
                - type: JSONPATH
                  parameters:
                    - '$.gpus["{#GPU_UUID}"].proc_count'
                - type: DISCARD_UNCHANGED_HEARTBEAT
                  parameters:
                    - 1h
              master_item:
                key: nvidia.apps.json
              tags:
                - tag: component
                  value: nvidia
                - tag: device
                  value: '{#GPU_UUID}'
          master_item:
            key: nvidia.apps.json
          preprocessing:
            - type: JAVASCRIPT
              parameters:
                - |
                  var input = JSON.parse(value);
                  var output = [];
                  for (var uuid in input.gpus) {
                      output.push({ "{#GPU_UUID}": uuid });
                  }
                  return JSON.stringify(output);
      tags:
        - tag: component
          value: nvidia
      macros:
        - macro: '{$GPU_PROC_MAX_AGE_LIMIT_INFO}'
          value: 3d
          description: 'Trigger at INFO level when a compute process has been running for this long.'
        - macro: '{$GPU_PROC_MAX_AGE_LIMIT_WARN}'
          value: 7d
          description: 'Trigger at WARNING level when a compute process has been running for this long.'
      valuemaps:
        - uuid: c868b183c2304444a94819f9fcbf01af
          name: 'Nvidia GPU compute status'
          mappings:
            - value: '0'
              newvalue: Idle
            - value: '1'
              newvalue: Active
            - value: '2'
              newvalue: 'Long-running process (info)'
            - value: '3'
              newvalue: 'Long-running process (warning)'
            - value: '4'
              newvalue: Error
